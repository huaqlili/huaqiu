---
layout: archive
title: ""
permalink: /publications/
author_profile: true
---

{% include base_path %}

Publications
======

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='../images/iclr25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="https://arxiv.org/abs/2503.14535" style="font-size: 22px; color: #483D8B; text-decoration: none">**Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios**</a><br>
<span style="font-size: 18px;">**Huaqiu Li**, Xiaowan Hu, Haoqian Wang†</span><br>
<span style="font-size: 18px;">[**Openreview**](https://openreview.net/forum?id=PVHoELf5UN&noteId=tWR79MUc4B)   [**Paper**](https://arxiv.org/abs/2503.14535)   [**Code**](https://github.com/huaqlili/unsupervised-light-enhance-ICLR2025)</span>

<span style="font-size: 18px;">- We propose an interpretable, zero-reference joint denoising and low-light enhancement framework tailored for real-world scenarios. Our method derives a training strategy based on paired sub-images with varying illumination and noise levels, grounded in physical imaging principles and retinex theory.</span>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='../images/promptsid.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="https://arxiv.org/abs/2502.06432" style="font-size: 22px; color: #483D8B; text-decoration: none">**Prompt-SID: Learning Structural Representation Prompt via Latent Diffusion for Single-Image Denoising**</a><br>
<span style="font-size: 18px;">**Huaqiu Li**\*, Wang Zhang\*, Xiaowan Hu, Tao Jiang, Zikang Chen, Haoqian Wang†</span><br>
<span style="font-size: 18px;">[**Paper**](https://arxiv.org/abs/2502.06432)   [**Code**](https://github.com/huaqlili/Prompt-SID)</span>

<span style="font-size: 18px;">-  In this paper, we introduce Prompt-SID, a prompt-learning-based single image denoising framework that emphasizes preserving of structural details. This approach is trained in a self-supervised manner using downsampled image pairs.</span>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='../images/spatio.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="https://arxiv.org/abs/2502.06432" style="font-size: 22px; color: #483D8B; text-decoration: none">**Spatiotemporal Blind-Spot Network with Calibrated Flow Alignment for Self-Supervised Video Denoising**</a><br>
<span style="font-size: 18px;">Chen, Zikang ; Jiang, Tao ; Hu, Xiaowan ; Zhang, Wang ; **Li, Huaqiu** ; Wang, Haoqian†</span><br>
<span style="font-size: 18px;">[**Paper**](https://ojs.aaai.org/index.php/AAAI/article/view/32242)   [**Code**](https://github.com/ZKCCZ/STBN)</span>

<span style="font-size: 18px;">-  We first explore the practicality of optical flow in the self-supervised setting and introduce a SpatioTemporal Blind-spot Network (STBN) for global frame feature utilization.</span>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICME 2025</div><img src='../images/icme.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="" style="font-size: 22px; color: #483D8B; text-decoration: none">**Measuring and Controlling the Spectral Bias in Self-Supervised Denoising**</a><br>
<span style="font-size: 18px;">Wang Zhang\*, **Huaqiu Li**\*, Tao Jiang, Zikang Chen, Haoqian Wang†</span><br>
<span style="font-size: 18px;">[**Paper**]()   [**Code**]()</span>

<span style="font-size: 18px;">-  We introduce a Spectral Controlling network (SCNet) to optimize self-supervised denoising of paired noisy images. First, we propose a selection strategy to choose frequency band components for noisy images, to accelerate the convergence speed of training.</span>

</div>
</div>

Preprints
======

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='../images/mmgenbench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="https://arxiv.org/abs/2411.14062" style="font-size: 22px; color: #483D8B; text-decoration: none">**MMGenBench: Fully Automatically Evaluating LMMs from the Text-to-Image Generation Perspective**</a><br>
<span style="font-size: 18px;">Hailang Huang, Yong Wang, Zixuan Huang, **Huaqiu Li**, Tongwen Huang, Xiangxiang Chu, Richong Zhang†</span><br>
<span style="font-size: 18px;">[**Paper**](https://arxiv.org/abs/2411.14062)   [**Code**](https://github.com/lerogo/MMGenBench)</span>

<span style="font-size: 18px;">-  We propose the MMGenBench-Pipeline, a straightforward and fully automated evaluation pipeline. This involves generating textual descriptions from input images, using these descriptions to create auxiliary images via text-to-image generative models, and then comparing the original and generated images.</span>

</div>
</div>
